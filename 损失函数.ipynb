{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1Loss 和 MSELoss损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "inputs = torch.tensor([1,2,3])\n",
    "\n",
    "targets = torch.tensor([1,2,5])\n",
    "\n",
    "inputs = torch.reshape(inputs,(1,1,1,3))\n",
    "\n",
    "targets = torch.reshape(targets,(1,1,1,3))\n",
    "\n",
    "# reduction设置为sum表示求和,默认为mean求和\n",
    "loss1 = L1Loss(reduction='sum')\n",
    "\n",
    "result = loss1(inputs,targets)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵主要用于衡量两个概率分布之间的差异。\n",
    "\n",
    "在分类问题里，一个是模型预测得到的概率分布，\n",
    "另一个是真实标签对应的概率分布（通常是一个 one - hot 编码的分布）。\n",
    "\n",
    "交叉熵的值越小，说明这两个分布越接近，也就意味着模型的预测越准确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross - Entropy Loss: 0.10536054521799088\n",
      "Multi - class Cross - Entropy Loss: 0.2395448386669159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 二分类示例\n",
    "criterion_binary = nn.BCELoss()\n",
    "# 真实标签\n",
    "y_binary = torch.tensor([1.0])\n",
    "# 模型预测概率\n",
    "p_binary = torch.tensor([0.9])\n",
    "loss_binary = criterion_binary(p_binary, y_binary)\n",
    "print(\"Binary Cross - Entropy Loss:\", loss_binary.item())\n",
    "\n",
    "# 多分类示例\n",
    "criterion_multi = nn.CrossEntropyLoss()\n",
    "# 真实标签索引\n",
    "y_multi = torch.tensor([0])\n",
    "# 模型输出的未经过 softmax 的分数\n",
    "scores_multi = torch.tensor([[3.0, 1.0, 1.0]])\n",
    "loss_multi = criterion_multi(scores_multi, y_multi)\n",
    "print(\"Multi - class Cross - Entropy Loss:\", loss_multi.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际数据10分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "tensor(2.3003, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3063, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2973, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2896, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3053, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3116, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3083, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3123, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3103, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2910, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3125, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3114, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2966, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3087, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3162, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2954, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3142, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2873, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3077, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3116, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3003, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3089, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2949, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3044, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3060, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3029, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3058, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2895, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2943, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3013, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2944, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3044, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2979, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3200, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3033, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3173, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2958, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3181, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3127, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3034, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3139, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3112, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3114, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2858, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3038, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2971, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3127, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2979, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3085, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3150, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3132, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3093, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3134, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2983, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2980, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3176, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3165, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3046, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3172, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3144, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3133, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3005, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2939, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3133, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2958, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3191, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3016, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3012, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3092, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3096, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3017, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3009, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2937, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3042, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3153, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2970, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3020, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3115, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3000, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3022, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2990, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2905, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2964, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3117, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2864, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2994, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3177, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3070, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2909, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3193, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3149, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3198, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2999, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3146, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3012, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2955, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2897, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3163, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3097, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3007, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3275, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3063, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3171, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3003, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3070, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3051, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3150, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3116, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3152, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2866, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2941, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2969, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2957, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3119, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3022, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3241, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3078, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2826, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3035, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2920, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3045, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3007, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3037, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2967, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3043, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3252, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Sequential,Conv2d,MaxPool2d,Flatten,Linear\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10('../data',train =False, transform = torchvision.transforms.ToTensor(),download = True)\n",
    "\n",
    "dataloader = DataLoader(dataset,batch_size = 64)\n",
    "\n",
    "class zzy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(zzy,self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "             Conv2d(3,32,5,padding=2),\n",
    "             MaxPool2d(2),\n",
    "             Conv2d(32,32,5,stride=1,padding=2),\n",
    "             MaxPool2d(2),\n",
    "             Conv2d(32,64,5,stride=1,padding=2),\n",
    "             MaxPool2d(2),\n",
    "             Flatten(),\n",
    "             Linear(64*4*4,64),\n",
    "             Linear(64,10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "       x = self.model1(x)\n",
    "       return x\n",
    "\n",
    "\n",
    "\n",
    "zzy1 =zzy()\n",
    "\n",
    "criterion_multi = nn.CrossEntropyLoss()\n",
    "\n",
    "for data in dataloader :\n",
    "    imgs ,targets = data\n",
    "    outputs = zzy1(imgs)\n",
    "    # 计算损失函数\n",
    "    result_loss = criterion_multi(outputs,targets)\n",
    "\n",
    "    print(result_loss)\n",
    "\n",
    "    result_loss.backward()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌐 整体结构：流水线协作的翻译团队\n",
    "Transformer像一个高效的翻译流水线，分为**编码器（Encoder）**和**解码器（Decoder）**两部分：\n",
    "- **编码器**：处理原始语言（比如中文），生成「理解向量」\n",
    "- **解码器**：根据编码器的理解，生成目标语言（比如英文）\n",
    "\n",
    "每个编码器和解码器内部都有多个「专家模块」（类似团队中的不同角色）。\n",
    "\n",
    "\n",
    "### 🔍 核心机制：自注意力（Self-Attention）\n",
    "- 在自己所处的环境中，要注意别人的动向。\n",
    "#### 比喻：会议讨论时的注意力分配\n",
    "当翻译团队处理句子「猫追老鼠」时：\n",
    "1. **分解任务**：每个词（猫、追、老鼠）都生成三个角色，即每个字都被映射为三个向量：\n",
    "   \n",
    "   - **查询（Query）**：我需要什么信息？\n",
    "   - **键（Key）**：我能提供什么信息？\n",
    "   - **值（Value）**：我实际包含的信息\n",
    "\n",
    "2. **计算关联度**：\n",
    "   - 每个词的Query会和其他词的Key计算「注意力分数」，通常使用点积运算（越相似点积结果越大）：\\(scores_{i,j} = q_i^T k_j\\)，为了避免点积结果过大，在计算得分时通常会除以 \\(\\sqrt{d_k}\\)：\\(scores_{i,j} = \\frac{q_i^T k_j}{\\sqrt{d_k}}\\)\n",
    "   - 应用 Softmax 函数：\n",
    "将得分通过 Softmax 函数转换为概率分布，得到***注意力权重***：\n",
    "\\(attention_{i,j} = \\frac{\\exp(scores_{i,j})}{\\sum_{l = 1}^{n} \\exp(scores_{i,l})}\\)\n",
    "   - 计算加权和：\n",
    "根据注意力权重对 Value 进行加权求和，得到每个位置的输出，即生成当前词的最终表示（比如：「追」的最终表示会包含「猫」的主动性和「老鼠」的移动方向）：\n",
    "\\(output_i = \\sum_{j = 1}^{n} attention_{i,j} v_j\\)综合起来，自注意力机制的输出可以表示为：\n",
    "***\\(Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\)***\n",
    "\n",
    "### Q、K、V矩阵的生成\n",
    "#### 1、生成Q矩阵\n",
    "使用权重矩阵 \\(W^Q \\in \\mathbb{R}^{d_{model} \\times d_k}\\) 对输入序列 X 进行线性变换，从而得到 Query 矩阵 Q：\n",
    "\\(Q = XW^Q\\)\n",
    "这里的 \\(Q \\in \\mathbb{R}^{n \\times d_k}\\)，n 是序列的长度，\\(d_k\\) 是 Query 向量的维度。\n",
    "- ***形象解释***： 把 \\(W^Q\\) 想象成一个 “提问专家”。对于会议中的每个人 \\(x_i\\)，“提问专家” \\(W^Q\\) 会根据这个人的发言内容 \\(x_i\\) 生成一个 \\(d_k\\) 维的问题 \\(q_i\\)。所有这些问题组合起来就构成了 Query 矩阵 Q。这个问题代表着该元素在寻找相关信息时所关注的方向。\n",
    "#### 2、生成 Key 矩阵 \n",
    "K数学公式使用权重矩阵 \\(W^K \\in \\mathbb{R}^{d_{model} \\times d_k}\\) 对输入序列 X 进行线性变换，得到 Key 矩阵 K：\n",
    "\\(K = XW^K\\)\n",
    "这里的 \\(K \\in \\mathbb{R}^{n \\times d_k}\\)。\n",
    "- ***形象解释***：把 \\(W^K\\) 想象成一个 “信息标签专家”。对于会议中的每个人 \\(x_i\\)，“信息标签专家” \\(W^K\\) 会根据这个人的发言内容 \\(x_i\\) 生成一个 \\(d_k\\) 维的标签 \\(k_i\\)。所有这些标签组合起来就构成了 Key 矩阵 K。这个标签表示该元素能够提供的信息特征。\n",
    "#### 3、生成 Value 矩阵 \n",
    "V数学公式使用权重矩阵 \\(W^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\) 对输入序列 X 进行线性变换，得到 Value 矩阵 V：\n",
    "\\(V = XW^V\\)\n",
    "这里的 \\(V \\in \\mathbb{R}^{n \\times d_v}\\)，\\(d_v\\) 是 Value 向量的维度，通常 \\(d_k = d_v\\)。\n",
    "- ***形象解释***：把 \\(W^V\\) 想象成一个 “信息提取专家”。对于会议中的每个人 \\(x_i\\)，“信息提取专家” \\(W^V\\) 会根据这个人的发言内容 \\(x_i\\) 提取出一个 \\(d_v\\) 维的信息片段 \\(v_i\\)。所有这些信息片段组合起来就构成了 Value 矩阵 V。这个信息片段是该元素实际包含的有价值信息。\n",
    "\n",
    "下图演示了这个过程：\n",
    "![3](Note_imags\\7.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 🕒 位置编码：给单词标上时间戳\n",
    "由于 Transformer 本身没有捕捉序列顺序信息的能力，因此需要通过位置编码来为输入序列中的每个位置添加位置信息。\n",
    "Transformer给每个词添加位置信息：\n",
    "- 不同位置的词有不同的「位置向量」\n",
    "- 就像电影票的座位号，确保模型理解「猫追老鼠」和「老鼠追猫」的顺序差异\n",
    "位置编码通常使用正弦和余弦函数来生成：\\(PE_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\)\\(PE_{(pos, 2i + 1)} = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\)其中 pos 是位置索引，i 是维度索引，\\(d_{model}\\) 是模型的维度。将位置编码 PE 与输入序列 X 相加，得到带有位置信息的输入：\\(X_{pos} = X + PE\\)\n",
    "\n",
    "\n",
    "### 🧠 多头注意力：多组专家同时分析\n",
    "#### 比喻：不同领域专家分别研究\n",
    "Transformer用**8组独立的注意力机制**（多头）：\n",
    "***原理理解***：\n",
    "- 每组关注不同的语义关系（如主谓、动宾、修饰关系）\n",
    "- 最后将各组结果合并，类似综合不同专家意见\n",
    "\n",
    "***计算步骤***\n",
    "设头的数量为 h，每个头的 Query、Key 和 Value 的维度为 \\(d_{k/h}\\) 和 \\(d_{v/h}\\)，满足 \\(d_k = h \\times d_{k/h}\\) 和 \\(d_v = h \\times d_{v/h}\\)。分割 Query、Key 和 Value：\n",
    "将 Q、K 和 V 分别分割成 h 个头：\n",
    "\\(Q_i = Q[:, :, i \\times d_{k/h}:(i + 1) \\times d_{k/h}]\\)\n",
    "\\(K_i = K[:, :, i \\times d_{k/h}:(i + 1) \\times d_{k/h}]\\)\n",
    "\\(V_i = V[:, :, i \\times d_{v/h}:(i + 1) \\times d_{v/h}]\\)\n",
    "其中 \\(i = 0, 1, \\cdots, h - 1\\)。计算每个头的注意力输出：\n",
    "对每个头分别计算自注意力：\n",
    "\\(head_i = Attention(Q_i, K_i, V_i)\\)拼接头的输出：\n",
    "将 h 个头的输出拼接起来：\n",
    "\\(concat = [head_0; head_1; \\cdots; head_{h - 1}]\\)线性变换得到最终输出：\n",
    "通过一个线性变换矩阵 \\(W^O \\in \\mathbb{R}^{h \\times d_{v/h} \\times d_{model}}\\) 对拼接后的结果进行变换：\n",
    "\\(MultiHead(Q, K, V) = concat W^O\\)\n",
    "\n",
    "### 解码器\n",
    "由多个相同的解码器层堆叠而成，每个解码器层包含三个子层：掩码多头注意力子层、编码器 - 解码器多头注意力子层和前馈神经网络子层。1. 掩码多头注意力子层为了防止解码器在生成序列时提前看到未来的信息，需要在多头注意力机制中使用掩码。掩码多头注意力的输出为 \\(MaskedMHA(Y)\\)，然后通过残差连接和层归一化：\n",
    "\\(Y_1 = \\text{LayerNorm}(Y + MaskedMHA(Y))\\)\n",
    "其中 Y 是解码器的输入。2. 编码器 - 解码器多头注意力子层解码器使用编码器的输出 \\(X_2\\) 作为 Key 和 Value，解码器自身的输出 \\(Y_1\\) 作为 Query，计算多头注意力 \\(MHA(Y_1, X_2, X_2)\\)，然后通过残差连接和层归一化：\n",
    "\\(Y_2 = \\text{LayerNorm}(Y_1 + MHA(Y_1, X_2, X_2))\\)3. 前馈神经网络子层与编码器中的前馈神经网络子层类似，计算 \\(FFN(Y_2)\\) 并通过残差连接和层归一化：\n",
    "\\(Y_3 = \\text{LayerNorm}(Y_2 + FFN(Y_2))\\)最终输出解码器的最终输出通过一个线性层和 Softmax 函数得到每个位置的概率分布，用于预测目标序列中的每个元素。\n",
    "\n",
    "### 🚀 优势：并行处理 vs 传统模型\n",
    "#### 对比：流水线 vs 接力赛\n",
    "- **LSTM**：必须按顺序处理每个词（像接力赛）\n",
    "- **Transformer**：所有词同时处理（像流水线）\n",
    "- 因此Transformer速度更快，且能捕捉长距离依赖（如相隔很远的两个词的关系）\n",
    "\n",
    "\n",
    "### 🌰 总结：翻译「猫追老鼠」的过程\n",
    "1. **编码器**：\n",
    "   - 每个词通过自注意力理解上下文关系\n",
    "   - 输出包含全局信息的「理解向量」\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 🌈 为什么Transformer如此强大？\n",
    "- **并行计算**：处理速度比LSTM快10倍以上\n",
    "- **长距离依赖**：能捕捉相隔千词的语义关系\n",
    "- **迁移能力**：预训练后微调即可用于各种NLP任务（如问答、文本生成）\n",
    "\n",
    "如果把语言理解比作拼图游戏，Transformer就像能同时看到所有拼图碎片，并快速找到最佳拼接方式的超级玩家。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![示例图片](Note_imags\\6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编码器和解码器\n",
    "![示例图片](Note_imags\\5.png)\n",
    "#### 有多个编码器和解码器，不过每个编码器解码器在更新参数时，不是一样的，不像RNN是共享的。\n",
    "![示例图片](Note_imags\\4.png)\n",
    "#### 原论文中transformer的结构图\n",
    "- 左边是编码器，右边是解码器\n",
    "![示例图片](Note_imags\\3.png)\n",
    "#### 对每个字所占的512维向量分别进行编码，偶数用sin，奇数用cos\n",
    "![示例图片](Note_imags\\2.png)\n",
    "#### 将位置编码后的向量与原向量相加得到新的\n",
    "![示例图片](Note_imags\\1.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer并不像RNN那样，一个一个词的依次处理，而是一起，没有时间，需要位置编码"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
